{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99051513-de8e-4513-bda4-9dac694e2823",
   "metadata": {},
   "source": [
    "- **Always open Framework documentation whenever you are working with it,,,,,to use up to date name of functions,method,class ot that framwwork**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cfc4ad5-b3ba-4736-94b2-a7b4a684061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51b6e478-0284-4f73-9332-bb3bf8999036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url: str)-> None:\n",
    "    document=SimpleWebPageReader(html_to_text=True).load_data(urls=[url])#This line reads and processes the content from the specified URL, converting it into a text format.\n",
    "    index=VectorStoreIndex.from_documents(documents=document)#This line creates an index from the loaded documents. The VectorStoreIndex class is responsible for storing the vectors derived from the documents.\n",
    "    \n",
    "    query_engine=index.as_query_engine()#The as_query_engine method is used to enable querying the stored vectors. The query method processes the query and retrieves the relevant information.\n",
    "    response=query_engine.query('What is KNN?')#same as above,,,it is query engine\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a67099a8-bb7e-4b58-845e-c4920a7a02e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\playing_with_genai\\Lib\\site-packages\\llama_index\\core\\embeddings\\utils.py:59\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model, callback_manager)\u001b[0m\n\u001b[0;32m     58\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m OpenAIEmbedding()\n\u001b[1;32m---> 59\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\playing_with_genai\\Lib\\site-packages\\llama_index\\embeddings\\openai\\utils.py:103\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.ibm.com/topics/machine-learning\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     document\u001b[38;5;241m=\u001b[39mSimpleWebPageReader(html_to_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mload_data(urls\u001b[38;5;241m=\u001b[39m[url])\n\u001b[1;32m----> 3\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     query_engine\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mas_query_engine()\n\u001b[0;32m      5\u001b[0m     response\u001b[38;5;241m=\u001b[39mquery_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat is KNN?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\playing_with_genai\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:145\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[0;32m    138\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[0;32m    139\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     transformations,\n\u001b[0;32m    141\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    143\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\playing_with_genai\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:71\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43membed_model_from_settings_or_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     85\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\playing_with_genai\\Lib\\site-packages\\llama_index\\core\\settings.py:274\u001b[0m, in \u001b[0;36membed_model_from_settings_or_context\u001b[1;34m(settings, context)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39membed_model\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\playing_with_genai\\Lib\\site-packages\\llama_index\\core\\settings.py:67\u001b[0m, in \u001b[0;36m_Settings.embed_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[1;32m~\\.conda\\envs\\playing_with_genai\\Lib\\site-packages\\llama_index\\core\\embeddings\\utils.py:66\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model, callback_manager)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI embedding model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsider using embed_model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisit our documentation for more embedding options: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.llamaindex.ai/en/stable/module_guides/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings.html#modules\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# for image multi-modal embeddings\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m embed_model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
     ]
    }
   ],
   "source": [
    "main(url='https://www.ibm.com/topics/machine-learning')#You can also give local path of any file,,on which you have to perform QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9be45d-887c-4a70-a18b-42fe9e724bcd",
   "metadata": {},
   "source": [
    "- llama index's SimpleWebPagereader and vectorStoreIndex uses Openai's GPT and embedding models,,,respectively\n",
    "- ie;Default Storage Location -> By default, if no specific storage backend is provided, the vectors will be stored in memory. This means they are temporarily held within the program's memory ()RAM) and will be lost when the program terminates. This in-memory storage is suitable for small-scale, temporary applications where persistence and scalability are not required.\n",
    "- Persistent or Remote Storage -> To store vectors persistently or remotely, you would need to configure the VectorStoreIndex to use a specific backend. This could involve setting up a connection to a local database, a cloud-based database, or a specialized vector database.\n",
    "- **from llama_index.vector_stores import PineconeVectorStore**\n",
    "\n",
    "- Assuming you have the necessary API key and index name\n",
    "- **vector_store = PineconeVectorStore(api_key=\"YOUR_API_KEY\", index_name=\"YOUR_INDEX_NAME\")**\n",
    "- **index = VectorStoreIndex.from_documents(documents=document, vector_store=vector_store)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5546ab-b990-4217-87ac-311fda177cb3",
   "metadata": {},
   "source": [
    "- An index in Pinecone is essentially a namespace or a container that holds a set of vectors. It provides the infrastructure to store, manage, and query these vectors.\n",
    "- index is nothing but,,,thing or number through which we can represent any value uniquelyy,,thus access easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fae9f-8429-4bef-a000-7c67eee3f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic RAG,,,,,,,,\n",
    "#where,,text is extracted and passed to openai LLM,,and get the refined answer/response...........\n",
    "#002_QA_System_Using_Llama_index_And_Gemini..you can see,,how to use gemini,,instead openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d064cb-b5c8-457a-82e9-71b5969bc458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a9a6a-dac7-4a72-ada9-d89a4002a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
