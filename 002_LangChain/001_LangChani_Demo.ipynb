{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4e763c-399d-407a-91f8-07d5a4fb4c8f",
   "metadata": {},
   "source": [
    "- It is most powerfull library in todays date,,to build GEN AI based APPLication\n",
    "- Provide various powerfull tool's to build gen AI application\n",
    "1. Prompt Templating\n",
    "2. Agents\n",
    "3. Chains\n",
    "4. Memory\n",
    "5. Doc Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c5580c-dd29-4c8d-8b29-12a8dbc074ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "#How keras is built over tensorflow\n",
    "#in same way langchain is built over diff-diff genai APi(ex- huggingface , openai , gemeini , a121lab , etc)....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75c75a4-bc87-4be1-9174-9ec5286d69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling openai's models from langchain\n",
    "#We can say Lanchain is Interface between Gen ai models(openai's model,googles model,A121 lab models...........etc)\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d5c6b-fcc5-4eea-9e9c-bbc57c738d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI( openai_api_key = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e038adc8-3623-4099-ba43-905d1a5ef936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero shot prompting or direct prompting\n",
    "prompt = \"can you tell me total number of country in asia? can you give me top 10 country name?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb396be-733a-40b2-95bf-c4553cb019d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clinet.predict(prompt).strip())\n",
    "#strip will remove the \\n from output................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a061df-7acb-4fb2-865a-693a9c60de7a",
   "metadata": {},
   "source": [
    "## 1. Promp Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebc81a2-edce-46d8-b9ad-9a903f8179b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42dbd72-bc14-4335-95d2-65bdd813291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36981b73-a823-4ef6-8cde-70e6e5c676c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_varibles = [\"city\"],\n",
    "    template = \"can you tell me the capital of {city}?\"\n",
    ")\n",
    "#tou can add multiple variables,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "676116a5-d965-4153-a386-f8609bad90df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can you tell me the capital of delhi?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_name.format(city=\"delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f564a117-c7af-49e4-9af4-680d74bf6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = prompt_template_name.format(city=\"delhi\")\n",
    "p2 = prompt_template_name.format(city=\"karnataka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4e31a-3506-4c49-8c50-8065948d9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.predict(p1).strip())\n",
    "print(client.predict(p2).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e14e2f-be06-47a6-8877-867d42994dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bd60ee-22b4-40b4-8c9a-b44e730aebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name2 = PromptTemplate.from_template(\"What is good name for a company that makes a {Product}\")#you can add multiple input variables,,like product,category..etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063df08f-3b01-4728-b058-0817d24ff741",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PromptTemplate.format() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m p3 \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_template_name2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m p3\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#case sensetive key,,,,,,,,,p3 = prompt_template_name2.format(product = \"toys\"),gives u error\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: PromptTemplate.format() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "p3 = prompt_template_name2.format(Product=\"toys\")\n",
    "p3\n",
    "#case sensetive key,,,,,,,,,p3 = prompt_template_name2.format(product = \"toys\"),gives u error\n",
    "#and it is positional argument,,,so dont make this ->p3 = prompt_template_name2.format(\"toys\"),,,,it will give u error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134629c7-70b1-4f3c-b5f7-88bac300c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.predict(p3).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4af596-82cd-4049-98ec-3cc7d5182f97",
   "metadata": {},
   "source": [
    "## 2. Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c0a0c-dad6-4560-9bef-9111387a6bba",
   "metadata": {},
   "source": [
    "- Any 3rd party Tool or API ,,we call it as agent.........bcz we are requesting some services from it (tool or api)\n",
    "- in simple words Agent is a person or thing to which you assign some work and he/it will do for you\n",
    "- **Agents Internally Uses CHAINING CONCEPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe68877-fbdb-4862-bcb0-7c147e171445",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = \"can ypu tell me who won the recent cricket world cup?\"\n",
    "client.predict(p4).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d1200-8889-42a6-a01f-1691fff94e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5 = \"can ypu tell me current GDP of india?\"\n",
    "client.predict(p5).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51312cb9-5535-486b-ad77-16be19636847",
   "metadata": {},
   "source": [
    "- giving above prompts to gen Ai models,,doesnt give you correct answer or up to date answer,,,bcz they are trained till particular date\n",
    "- In that case u can use 3rd party API or tools,,to get answersss...\n",
    "- for Extracting **Real Time** info,,i am going to use **SERP API**\n",
    "- now by using this serp API i will call **google-search-engine**\n",
    "- and i will extract info in real time\n",
    "- serp api is similar to **Rapid APi**,,,,,,,,,,,,,,where you get all famous websites(service providing website's) API,,,,u can use them to collect real time data\n",
    "- using this serp api you can access Google search engine,wikipedia,youtube search api....etc to collect data\n",
    "- serp api linke -> https://serpapi.com/\n",
    "- first 100 searches are free....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb06b45-db1e-43a6-9b82-a4545352a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "serp_api_key = \"463ad4f208d8cfda4c484ae2f21fe5083ff0c6d317796d39edf0809e454d6be0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a811eb-0739-49c0-b0ce-c7ba1dfd1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5b5aa-212d-4e71-b529-99a29150335c",
   "metadata": {},
   "source": [
    "#### a) calling google search engine through serp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9415fbac-3ad5-40f3-8df8-67e5af06bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fc8a1-63a3-45f4-ab62-a867f78cc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAi(openai_api_key = openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93545a07-b37f-473c-871e-adaff19bd93d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tool \u001b[38;5;241m=\u001b[39m load_tools([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserpapi\u001b[39m\u001b[38;5;124m\"\u001b[39m] , serpapi_api_key \u001b[38;5;241m=\u001b[39m serp_api_key , llm\u001b[38;5;241m=\u001b[39m\u001b[43mclient\u001b[49m)\n\u001b[0;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m intialize_agent(tool , client , agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION , verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#verbose=True--> means,,whatever task is going on in backend while calling serp api to collect data is shown......\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "tool = load_tools([\"serpapi\"] , serpapi_api_key = serp_api_key , llm=client)\n",
    "\n",
    "agent = intialize_agent(tool , client , agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION , verbose=True)\n",
    "#verbose=True--> means,,whatever task is going on in backend while calling serp api to collect data is shown......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45025e42-dfa9-469a-8360-af88ec7340aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"can you tell me who won the world cup recently?\")\n",
    "#Now we get accurarate answers\n",
    "#ans as u see,,all backend  processing(ie;generally google search engine is called internally by serp api) is shown   (verbose-True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9520412-620e-48c7-b66b-86d9577da87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a70c8448-804a-42ef-aa31-bbb6a731b3a9",
   "metadata": {},
   "source": [
    "#### b) access wikepedia through serp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129c0db5-5347-46b6-925d-9b91fce4c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca2e59-896b-4a65-8795-c5a9cadf3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools([\"wikipedia\"],llm=client)\n",
    "agent = intialize_agent(tool , client , agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION , verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc13a56-ed6e-4d33-b4f8-9c6134d73ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aget.run(\"current GDP of india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2199c215-a817-41b9-8f09-10ec083d601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is use of mentioning llm in agent................internally chaining is done in agent code..ie; prompt is connected to LLM -> if llm doesnt have proper answer \n",
    "#then agent will use 3rd party api as we discussed above,,and search every search-engine untill it gets desired result -> then return it to llm -> \n",
    "#llm will parse it and display it w.r.t asked question / prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352b17a-341e-4f79-943c-a67a23664064",
   "metadata": {},
   "source": [
    "- if you just mention serpapi instead wikipedia or other any search engine...................then serp api will internally ,visit all search engine's untill it gets 1st expected result...............ie;internally it is uses chaining concept............\n",
    "- **agent.run(\"can you tell me who won the world cup recently?\") ............. if u run this code and bcz verbose=True,,you will see  backgrpund process,,,,,in that you can see,,how serp api will visiting one search engine and trying to get result,,if not obtain then goto other search engine,untill it gets desired result for prompt,,,,,,,,,,,,each and every backend process is shown clearly,,,,,,,,,,and this is nothing but Chaining**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f57bf-53a0-402e-ab70-3b7252890e19",
   "metadata": {},
   "source": [
    "## 3. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daff52bc-e554-4bdd-bbfa-deeab53cbc45",
   "metadata": {},
   "source": [
    "- **single chain means connecting prompt to gen ai model**\n",
    "- ##### sequential chain means collection of component in definend order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b93dc7-b473-4369-bbcf-22567f63c58a",
   "metadata": {},
   "source": [
    "- in real world chain is used to tie-up components together,,,,,,ex-> i can use chain to tie-up my bicycle with fixed_object(rod)\n",
    "- ex2-< to tie-up bags with seats,,in railway\n",
    "- In simple words,,chain is thing from which we can connect or tie  components/elements together...\n",
    "- ie; **chain is thing which connects several components**\n",
    "- **technical terms** -> Using An LLM in isolation(using just 1 llm api,,,which is not connected with other llm api in same application) is fine for simple applications , but more complex application requires Chaining LLM's either with each other or with other component(3rd party API,,,like serpapi,rapid api,...etc)\n",
    "- U can use any no of Gen AI models(models api) in to you single application ie;you can use gpt3.5 for doing 1 task , gpt-4 for another task , gemini for another task , mistral for one task , palm for one task...etc(just cost to use these model will increase),,,,,,,,,ie; u can use N no of Gen Ai models into you application and assigning them a diff-diff tasks,,,,,,,,,,,,,,,,,,,here we are not connected gen Ai model with each other,,,,,,,,,thus we can say **Gen Ai in isolation**\n",
    "- But if you want to connect Gen ai models togeteher (ie;output of gpt3.5 will be input to gemini and output of gemini is input to gpt4 ,,,,so on)then we have to use **CHAINING CONCEPT** in our application.......or u want to connect gen Ai model's to 3rd party APIs or tool, like we did above in Agents,,,,,,,,,,,,,(agents internally uses chaining,,,no need to mention externally as we do while connecting gen Ai models)\n",
    "- **in Agents(agent.run(\"\")),,,,,,,,we give prompt to gen Ai model -> if it doesnt have correct or real time answer then it will call agent(if implemented as above) -> then agent will search in diff-diff search engines untill it gets first result -> after getting result it will returned to llm and it will display output**\n",
    "- - **components means either Gen Ai models OR any3rd party API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a4389-f5d4-442e-8f44-a68a569ff137",
   "metadata": {},
   "source": [
    "#### a) ordinary chain concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15767105-def8-4010-904a-43c2dac5d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example1\n",
    "# below chain is constructed using 2 components,,,one is prompt and 2nd is llm model(gpt4o mini )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14852049-bc99-4a57-896a-e0c402a38f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI( openai_api_key = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a47d3-0b14-4d22-be25-e4035b4af752",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"what is good name for a company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd46dc02-3b95-4b4d-ae39-e283a98ef023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484893f-bfe8-4dc6-b78f-9c4f358ef9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client , prompt = prompt)\n",
    "#Prompt is connected to LLm models(client representing llm model........bcz in client we used gpt40 mini mode,,so this prompt is related to gpt4o mini)\n",
    "#and this is called chain OR single chain\n",
    "#and connecting such a several chain togeteher in particular order is called seuqntial chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfa3f9-66dc-4d1c-87a7-84ded40681a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Wine\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2d914-c205-4358-9b43-cbf1bde5bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184d6198-6d2c-4e26-80ba-900aec6250bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['cuisine'], template='i want to open a restaurant for {cuisine} food, suggest a fency name for this?')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_varibles = [\"cuisine\"],\n",
    "    template = \"i want to open a restaurant for {cuisine} food, suggest a fency name for this?\"\n",
    ")\n",
    "prompt_template_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bafafb-3fe2-40d0-8310-ef66297c35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain ( llm = client , prompt = prompt_template_name , verbose = True)\n",
    "chain.run(\"indian\")\n",
    "#as u see below,,,all backend process is shown,,,,,,,,,,,,,entered chain by formatting the prompt then given to llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6b04b-2e4c-4773-bfdb-57734b8e613e",
   "metadata": {},
   "source": [
    "#### b)SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5a3dc-1215-429c-9ae1-b6eea798d54f",
   "metadata": {},
   "source": [
    "- if we want to combine multiple chains and set a sequence for that (ie; after this chain this one is executed then this..ike this)\n",
    "- **1/single chain means input/prompt connected to 1 gen ai model ,,,,, one gen ai model connected with another one gen ai model ,**\n",
    "- multiple means ,,,, single chains connected with other single chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "981d3e13-f438-4e30-9a5e-7d7bd896637b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#################################################################################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m prompt_template_name1 \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m      3\u001b[0m     input_varibles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup_name\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      4\u001b[0m     template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi want to start a startup for \u001b[39m\u001b[38;5;132;01m{startup_name}\u001b[39;00m\u001b[38;5;124m, suggest me a good name for this?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m name_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m , prompt \u001b[38;5;241m=\u001b[39m prompt_template_name1) \u001b[38;5;66;03m# single chain 1\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#################################################################################################\u001b[39;00m\n\u001b[0;32m     10\u001b[0m prompt_template_name2 \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     11\u001b[0m     input_varibles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     12\u001b[0m     template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggest some stratergies for  \u001b[39m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "#################################################################################################\n",
    "prompt_template_name1 = PromptTemplate(\n",
    "    input_varibles = [\"startup_name\"],\n",
    "    template = \"i want to start a startup for {startup_name}, suggest me a good name for this?\"\n",
    ")\n",
    "name_chain = LLMChain(llm = client , prompt = prompt_template_name1) # single chain 1\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "prompt_template_name2 = PromptTemplate(\n",
    "    input_varibles = [\"name\"],\n",
    "    template = \"suggest some stratergies for  {name}\"\n",
    ")\n",
    "stratergies_chain = LLMChain(llm = client , prompt = prompt_template_name2) # single chain 2\n",
    "#u can use any other llm models also here,,not compulsory that u have to use same llm models as u used above,,,,,,,,,,,,,,,,,,\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eac2b1-bbaa-4743-9f18-f3439dad78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If u not mention below code then above code is just a isolation of Gen ai models,,ie;u have to individualy give input to both models,,,,,,,,,,,,,,,,\n",
    "#ie; name_chain.run(startup_name=\"AI\"),,,,,,,,,,,,,,,stratergies_chain.run(name=\"AI wizards\")...like this...u have to run each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b7c7c-a33d-4301-b2ad-3756d7ba51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#but if u want to connect above chains,,,such that ,,, u have to provide just 1 input,,then automatically output of first prompt/gen ai model \n",
    "#is given as input to next gen ai model............"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3c38ff5-30f1-4dc1-8f03-42b575d59e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fb7bf3e-0cc8-479b-9902-632e1ab66309",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chain \u001b[38;5;241m=\u001b[39m SimpleSequentialChain( chains\u001b[38;5;241m=\u001b[39m[\u001b[43mname_chain\u001b[49m , stratergies_chain])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'name_chain' is not defined"
     ]
    }
   ],
   "source": [
    "chain = SimpleSequentialChain( chains=[name_chain , stratergies_chain])\n",
    "#in chains parameter sequence matters...............ie; argument from chain.run(\"Artificial intelligence\") is passed to name_chain,,\n",
    "#then output of this chain is given to stratergies_chain\n",
    "#[name_chain , stratergies_chain],,,,ordering is matters,,,,,,,,,,,[stratergies_chain , name_chain],,,this, then arguments from  chain.run(\"Artificial intelligence\")\n",
    "#is passed to stratergies_chain,,,and output of this prompt or gen ai model is give as input to name_chain prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba98e5d-9e9a-4c25-9fa3-65b6b9216f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Artificial intelligence\")\n",
    "#as u see we get,,,direct result for last gen ai model or last prompt...................intermediate output is not shown,,,,,\n",
    "#just give to next sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281b97e-905e-4dd5-a4ac-506cd6968629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if u want to display or track,,intermediate output also,,then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1c5d9-1103-4069-a21f-50fc3a11d515",
   "metadata": {},
   "source": [
    "##### See How Sequential chain works,,,ie;by showing intermediate results by storing it in variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8a646-b03c-4c98-b1c4-552c5b5ee754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "prompt_template_name1 = PromptTemplate(\n",
    "    input_varibles = [\"cuisine\"],\n",
    "    template = \"i want to open a restaurant for {cuisine}, suggest a fency name for it\"\n",
    ")\n",
    "name_chain = LLMChain(llm = client , prompt = prompt_template_name1 , output_key=\"restaurant_name\") # output for this prompt is stored in \"restaurant_name\" this variable \n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "prompt_template_name2 = PromptTemplate(\n",
    "    input_varibles = [\"restaurant_name\"],\n",
    "    template = \"suggest some menu items for {restaurant_name}\"\n",
    ")\n",
    "food_items_chain = LLMChain(llm = client , prompt = prompt_template_name2 , output_key = \"menu_items\") #output for this prompt is stored in \"menu_items\" this variable \n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b3462-f817-4e73-9f44-fe2b68260710",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = SequentialChain(\n",
    "    chains=[name_chain , food_items_chain],\n",
    "    output_variables = [\"restaurant_name\",\"menu_items\"]\n",
    ")\n",
    "#note sequence matters in chains=[name_chain , food_items_chain] , output_variables = [\"restaurant_name\",\"menu_items\"]  these parametes list values\n",
    "#output of bame_chain is stored in restaurant_name and given as input to food_items_chain , output of food_items_chain is stored in menu_items......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc250a-01cf-4336-92ba-97b4600ce052",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain({\"cuisine\":\"indian\"})\n",
    "#you will get entire result in dictionary format,,,,ie;each prompt / gen ai models which input and generates which output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9acad-0046-414e-a6c5-d95eb1c07d75",
   "metadata": {},
   "source": [
    "- chatgpt application also build using this kind of chaining concept,,,,,,,,,,,,,,,,,,,,,,,,thus it mentains context and answer accordingly\n",
    "- If u want to retain info from first prompt to last prompt then utilize Sequential chain concept"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e4794ab-e5ef-4cc8-8928-bf6905933a1c",
   "metadata": {},
   "source": [
    "- Sequential Chain:\n",
    "SimpleSequentialChain: Executes a sequence of steps one after another.\n",
    "SequentialChain: Allows more complex sequences with branching and looping.\n",
    "\n",
    "- MapReduce Chain:\n",
    "Combines the MapReduce paradigm with chaining. The input is mapped to multiple sub-chains, and their outputs are reduced to a final result.\n",
    "\n",
    "- Conditonal Chain:\n",
    "Executes different chains based on certain conditions or criteria.\n",
    "\n",
    "- Parallel Chain:\n",
    "Runs multiple chains in parallel and combines their outputs at the end.\n",
    "\n",
    "- LSTM Chain:\n",
    "Uses Long Short-Term Memory (LSTM) networks to remember previous interactions and generate responses.\n",
    "\n",
    "- GPT Chain:\n",
    "Leverages Generative Pre-trained Transformers (GPT) to generate responses based on previous interactions.\n",
    "\n",
    "- Retrieval-Augmented Generation (RAG) Chain:\n",
    "Combines retrieval-based methods with generative models to generate more informed and contextually relevant responses.\n",
    "\n",
    "- Hierarchical Chain:\n",
    "Organizes chains hierarchically, where higher-level chains control the execution of lower-level chains.\n",
    "\n",
    "- Meta Chain:\n",
    "Chains that involve meta-reasoning or self-reflection, allowing the model to think about its own thought process.\n",
    "\n",
    "- Memory-Enhanced Chain:\n",
    "Incorporates external memory mechanisms to keep track of long-term context beyond the immediate conversation window.\n",
    "\n",
    "- Feedback Loop Chain:\n",
    "Involves feedback mechanisms where the system's responses are evaluated and refined based on user feedback.\n",
    "Hybrid Chain:\n",
    "\n",
    "Combines multiple chaining techniques to leverage the strengths of different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273deb4-f2e9-4447-bb62-9ce07e8fc2b1",
   "metadata": {},
   "source": [
    "## 4.Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8989311e-f286-44ac-86ae-a4d2f05652ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411b2b65-f867-45c8-a03c-a091f6065862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd6065e-f2e3-4266-b5e5-1686341bba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"C:\\Users\\shree\\Desktop\\education_task.pdf\")\n",
    "#loader is langchain object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5553ead7-775e-4720-b356-bdf4269a4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8770cf5-1b6a-4d1f-8648-07ddac469f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\shree\\\\Desktop\\\\education_task.pdf', 'page': 0}, page_content='Recommended Open -Source/Unpaid LLM Models  \\n1. BLOOM by BigScience  \\n• Description : BLOOM (BigScience Large Open -science Open -access Multilingual \\nLanguage Model) is a multilingual model designed to perform well across various \\nlanguages and tasks.  \\n• SuperGLUE Score : ~78.5  \\n• MMLU Score : ~70.0  \\n• Strengths : Multilingual capabilities, open -access nature, strong performance in \\nsummarization tasks.  \\n• Link : BLOOM on Hugging Face  \\n2. GPT -NeoX by EleutherAI  \\n• Description : GPT -NeoX is an open -source LLM developed by EleutherAI, known \\nfor its performance in generating human -like text.  \\n• SuperGLUE Score : ~72.3  \\n• MMLU Score : ~63.2  \\n• Strengths : Good performance on various NLP tasks, strong community support, \\nscalability.  \\n• Link : GPT -NeoX on Hugging Face  \\n3. GPT -J by EleutherAI  \\n• Description : GPT -J is an open -source model from EleutherAI with strong \\nperformance in various language tasks.  \\n• SuperGLUE Score : ~67.3  \\n• MMLU Score : ~55.6  \\n• Strengths : Strong performance in generating coherent text, cost -effective.  \\n• Link : GPT -J on Hugging Face  \\n4. T5 by Google  \\n• Description : T5 (Text -to-Text Transfer Transformer) is a versatile model that can be \\nfine-tuned for various NLP tasks, including summarization.  \\n• SuperGLUE Score : ~89.3 (for T5 -XXL, the largest variant)  \\n• Strengths : High performance, versatility in handling different tasks, effective in \\nsummarization.  \\n• Link : T5 on Hugging Face  \\n6. RoBERTa by Facebook AI  \\n• Description : RoBERTa is an optimized version of BERT.  \\n• SuperGLUE Score : ~89.9 (for RoBERTa -large)  \\n• Link : RoBERTa on Hugging Face  \\n7. ALBERT by Google Research  \\n• Description : ALBERT is a smaller and more efficient version of BERT.  \\n• SuperGLUE Score : ~89.3 (for ALBERT -xxlarge)  \\n• Link : ALBERT on Hugging Face  \\n8. FLAN -T5 by Google Research'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\shree\\\\Desktop\\\\education_task.pdf', 'page': 1}, page_content='• Description : FLAN -T5 is a family of models trained on a mixture of natural language \\nunderstanding and generation tasks.  \\n• SuperGLUE Score : ~89.3 (for FLAN -T5-XXL)  \\n• Link : FLAN -T5 on Hugging Face  \\n \\nAnalysis  \\n• Performance : T5,RoBERTa,ALBERT,  especially the larger variants, offers the \\nhighest performance but requires more computational resources. BLOOM and GPT -\\nNeoX provide a balance of performance and accessibility.  \\n• Cost and Availability : All recommended models are free and open -source, ensuring \\nno cost for API usage. Computational costs can be managed by using efficient cloud \\nservices or local deployment.  \\n• Suitability for Summarization : T5 and BLOOM are particularly strong in text \\ngeneration and summarization tasks, making them ideal for our use case.  \\nConclusion  \\nFor the educational application aimed at summarizing student performance, the following \\nmodels are recommended based on their performance, cost -effectiveness, and suitability:  \\n1. T5 \\n2. RoBERTa  \\n3. ALBERT  \\n \\nRecommended Cloud  \\nhttps://github.com/shreedhar13/OpenLLM  \\nCloud – K8S , BentoCloud')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35ae972d-5d23-4d6b-88d0-15fc4621f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)\n",
    "#as u see,,,this pdf has actually 2 pages in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6944f774-5ce3-4876-b89b-af345c303522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\shree\\\\Desktop\\\\education_task.pdf', 'page': 0}, page_content='Recommended Open -Source/Unpaid LLM Models  \\n1. BLOOM by BigScience  \\n• Description : BLOOM (BigScience Large Open -science Open -access Multilingual \\nLanguage Model) is a multilingual model designed to perform well across various \\nlanguages and tasks.  \\n• SuperGLUE Score : ~78.5  \\n• MMLU Score : ~70.0  \\n• Strengths : Multilingual capabilities, open -access nature, strong performance in \\nsummarization tasks.  \\n• Link : BLOOM on Hugging Face  \\n2. GPT -NeoX by EleutherAI  \\n• Description : GPT -NeoX is an open -source LLM developed by EleutherAI, known \\nfor its performance in generating human -like text.  \\n• SuperGLUE Score : ~72.3  \\n• MMLU Score : ~63.2  \\n• Strengths : Good performance on various NLP tasks, strong community support, \\nscalability.  \\n• Link : GPT -NeoX on Hugging Face  \\n3. GPT -J by EleutherAI  \\n• Description : GPT -J is an open -source model from EleutherAI with strong \\nperformance in various language tasks.  \\n• SuperGLUE Score : ~67.3  \\n• MMLU Score : ~55.6  \\n• Strengths : Strong performance in generating coherent text, cost -effective.  \\n• Link : GPT -J on Hugging Face  \\n4. T5 by Google  \\n• Description : T5 (Text -to-Text Transfer Transformer) is a versatile model that can be \\nfine-tuned for various NLP tasks, including summarization.  \\n• SuperGLUE Score : ~89.3 (for T5 -XXL, the largest variant)  \\n• Strengths : High performance, versatility in handling different tasks, effective in \\nsummarization.  \\n• Link : T5 on Hugging Face  \\n6. RoBERTa by Facebook AI  \\n• Description : RoBERTa is an optimized version of BERT.  \\n• SuperGLUE Score : ~89.9 (for RoBERTa -large)  \\n• Link : RoBERTa on Hugging Face  \\n7. ALBERT by Google Research  \\n• Description : ALBERT is a smaller and more efficient version of BERT.  \\n• SuperGLUE Score : ~89.3 (for ALBERT -xxlarge)  \\n• Link : ALBERT on Hugging Face  \\n8. FLAN -T5 by Google Research')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "563d7389-a46b-4851-8eb5-76ea7a350f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\shree\\\\Desktop\\\\education_task.pdf', 'page': 1}, page_content='• Description : FLAN -T5 is a family of models trained on a mixture of natural language \\nunderstanding and generation tasks.  \\n• SuperGLUE Score : ~89.3 (for FLAN -T5-XXL)  \\n• Link : FLAN -T5 on Hugging Face  \\n \\nAnalysis  \\n• Performance : T5,RoBERTa,ALBERT,  especially the larger variants, offers the \\nhighest performance but requires more computational resources. BLOOM and GPT -\\nNeoX provide a balance of performance and accessibility.  \\n• Cost and Availability : All recommended models are free and open -source, ensuring \\nno cost for API usage. Computational costs can be managed by using efficient cloud \\nservices or local deployment.  \\n• Suitability for Summarization : T5 and BLOOM are particularly strong in text \\ngeneration and summarization tasks, making them ideal for our use case.  \\nConclusion  \\nFor the educational application aimed at summarizing student performance, the following \\nmodels are recommended based on their performance, cost -effectiveness, and suitability:  \\n1. T5 \\n2. RoBERTa  \\n3. ALBERT  \\n \\nRecommended Cloud  \\nhttps://github.com/shreedhar13/OpenLLM  \\nCloud – K8S , BentoCloud')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6761f6-4275-4f11-9a89-d016b6688122",
   "metadata": {},
   "source": [
    "- **explore Langchain documentation,,,you will get everything,,,,,,no need to meorize everything**\n",
    "- load .txt,.pdf, .xlsx , .csv ..etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afea44-0fac-4039-b9df-220afb3a3fb3",
   "metadata": {},
   "source": [
    "## 5. Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87652d1d-e71e-4ca8-9f25-cb106db88113",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_varibles = [\"product\"],\n",
    "    template = \"what is a good name for a company that makes {product}\"\n",
    ")\n",
    "chain = LLMChain(llm = client , prompt = prompt_template_name ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d375f-d1c9-47fd-9a53-15c793c98f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chian.run(\"colorful cup\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd054e05-bfe3-4c9a-9597-c0ecc5677c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.memeory\n",
    "#memory parameter or Attribute present in LLMchain class\n",
    "#as u see it doesnt have anything,,,bcz we have not intialized it and set it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86731972-8449-42d3-88fa-16db4cf24009",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(chain.memory)\n",
    "#None type,,,,,,,,,,,,,ie; not retained any memory or conversation like chatgpt do..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8e149-cdd6-4c76-932e-d9cc1db9f08c",
   "metadata": {},
   "source": [
    "#### a) ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0d9a7-c768-48f8-bcd9-c829e3174fff",
   "metadata": {},
   "source": [
    "- sustain the conversation memory............\n",
    "- we attach the memory to LLM chain,,so that it can retain context of conversation\n",
    "- for predefined prompt template we use this class,,,,,,,,but in real world we dont use this,,,,,,there is no sence in maintaining memory for this,,,,,,,,,,,,think you will get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144f3bc7-7c37-4bc0-85d7-e51f08def9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10b24c2-0bfb-4c6b-ad32-9058b86396a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235986e-e1f3-42ac-a728-52406f16cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_varibles = [\"product\"],\n",
    "    template = \"what is a good name for a company that makes {product}\"\n",
    ")\n",
    "chain_with_memory = LLMChain(llm = client , prompt = prompt_template_name , memory=memory) \n",
    "#as u mentioned memory object in LLMChain parameters,,,,,,,,,now this chain_with_memory object or single chain has memory buffer attached to this,,,\n",
    "#thus retain ALL conversation happend through this object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bacc07-97fe-4f12-b39c-d8c4366df211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So using this object chain_with_memory,,whatever conversation we do is stored in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534db58-7b08-4a05-9d2a-9548b20e8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory.run(\"wines\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154a080-3216-4e3b-b621-fbfb96376c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory.run(\"camera\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004f610-1070-4834-a923-13fb75feac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory.run(\"drone\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71106e9-aa3a-4a63-a521-f2c751c98418",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_memory.memory\n",
    "#as u see ,,it is not empty,,,,,,,,,,,,,,,it contains some data,,,let see what it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770082b9-2863-458c-a293-b28f8edf76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain_with_memory.memory.buffer)\n",
    "#As u see ,, all conversation b/n human and genai model(llm=client) through chain_with_memory object is retained in buffer memory of API side \n",
    "#ie;gen ai model to which we reffered "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8516ecbe-daa4-4c77-b73b-4aced9e13587",
   "metadata": {},
   "source": [
    "- maintained all previous conversation in memory\n",
    "- The ConversationBufferMemory component in LangChain (a popular framework for building applications with language models) stores conversation history on the API side. This means that the memory of past interactions is maintained in the context of the language model's session rather than being saved locally on the user's device.\n",
    "- In practical terms, the conversation history is managed by the server handling the language model, allowing the model to access and reference previous exchanges to maintain context and coherence in ongoing interactions. This approach ensures that memory is preserved across API calls without requiring local storage solutions on the client side.\n",
    "- The context window size for OpenAI models refers to the maximum number of tokens (words or characters) the model can process in a single input. This is different from the concept of ConversationBufferMemory but related in terms of how they impact usage and cost.\n",
    "- **conversationbuffermemory grows endlessly,,,,,,,,,,,,,,,but APi side organization will charge you for it,,,so dont waste it**\n",
    "- **1 or single chain means (1 prompt connected with 1 gen ai model )  OR (1 gen ai model connected with another one gen ai model)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27bbe3-f984-4c41-8dca-5ed6ef9fd830",
   "metadata": {},
   "source": [
    " #### b) conversationchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9152ce70-1907-47e4-98d1-3b982fa24b98",
   "metadata": {},
   "source": [
    "- As u see above,,we just mantain conversation memory for particular prompt template,,,,,,,,,,but it not the real world scenario,,,,,,,,in real world scenario w/o any template conversation happen,,so this class is introduced\n",
    "- no prompt template,,,,,,**conversation is not predefined,,it is random**\n",
    "- ie; **Same as we do chating with chatGPT application**,,,,,,,\n",
    "- thus,,this is used in chat based application\n",
    "- **Like converationbufferedmemory , conversationchain memroy also grows endlessly**\n",
    "- if we want same behavior from gen ai models like chatGPt then this memory class is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a81e6d3-67db-481d-bb38-334bf4bf7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d214866-01a4-45f0-ad8a-02f84a5fcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationChain( llm = OpenAI(openai_api_key=API_KEY , temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b94d7a-56be-4a7d-9b0b-44a3a28b76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"who won the 1st cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f068a14-2ee9-40e8-b0c9-a4badb3eb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"what is 2+2 = ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a6817-a60c-48f7-86e0-3a0a82b4e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"what are prime numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb62ecd-4f2b-4e2c-a651-4af49ca41705",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"who was the captain of winning team?\")\n",
    "#as u see,,i dont mention which team and which sports,,,,,,,,then also it gives me correct answer,,,bcz it sustained the context memory\n",
    "#bcz in 1st prompt we asked about cricket world cup,,so it will reffer it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022d96f-206d-4395-a191-3750cad245d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"what is answer when we divide above number by 2\")\n",
    "#reffered to 2+2,,thus answer is 4/2 => 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce81cb8-0703-494d-b45a-61abbb8aebf6",
   "metadata": {},
   "source": [
    "#### c) ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3537a83-39d9-4829-84c5-ed9b3cf0d016",
   "metadata": {},
   "source": [
    "- **memory in both conversationbufferwindow and conversation chain grows endlessly,,,,,,,,,,which cause more cost,,,thus to apply contsrain on size of sustained memory,,we introduced ConversationBufferWindowMemory**\n",
    "- For just retaining/rememebering last 5 conversation or last 10 to 20 **conversation chain or interaction**,,,,,,,,,,then use **ConversationBufferWindowMemory**\n",
    "- **This class is used to declare window size for memory or buffer,,after that conversationbuffermemory ot conversationchain is used for further process**\n",
    "- w/o this class,,above 2 methods are sustain conversation endlessly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d13c7c-7781-4b39-9bcd-1cb6e0f7a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde75d2a-49eb-4448-a8c7-49e9b37e93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245e4f9-a1a0-422a-b5f5-e2dd34877c52",
   "metadata": {},
   "source": [
    "- k = 1: The system remembers only the last exchange (the most recent question and answer pair).\n",
    "- k = 2: The system remembers the last two exchanges (the two most recent question and answer pairs).\n",
    "- k = N: The system remembers the last N exchanges.\n",
    "- model stores or**Rememebers LAST K interactions (Prompt + its output)** in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2b209-0283-4d17-ae38-849763989538",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationChain( llm = OpenAI(openai_api_key=API_KEY , temperature=0.7) , memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6beef17-4b35-46a9-b32f-dceceb24deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"who won the 1st cricket world cup?\")#empty memory,,,bcz this is start of conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8fac4-bac8-43ca-80b2-f69f1209244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"what is 2+2 = ?\")#rememebers the convo.run(\"who won the 1st cricket world cup?\")  and  its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6bd77-0104-4b4f-879f-aa4ba40cf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"who was the captain of winning team?\")#Gives incorrect answer,,,,,,,,,\n",
    "#bcz we added constrain to this llm model that,,,it has to remember just most recent or last prompt related data(ie;last prompt/input and its output resepectively)\n",
    "#in this case last prompt and completion pair is (what is 2+2=? and answer/completion related to it)\n",
    "#thus cant remember this convo.run(\"who won the 1st cricket world cup?\") prompt and answer related to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43518cc8-1c67-49c2-be46-08fa006915e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"what are prime numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a068f-8c50-4c47-9664-9ded3c67b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"who was the captain of winning team?\")##Gives incorrect answer,,,,,,,,,bcz it remembers onlys last prompt and its answer ie; what are prime no and its oytput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fa170-7de1-4bdc-ac88-b4539c73a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"what is answer when we divide above number by 2\")#Gives incorrect answer,,,,,,,,,bcz it maintained last interaction(prompt and its output) \n",
    "#ie; convo.run(\"who was the captain of winning team?\" and its answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d39f1a-6fb8-4146-9833-877435d220a8",
   "metadata": {},
   "source": [
    "### NOTE= u can also save the particular data in context memory / buffer ,,,,ex-> any pdf,csv,txt,html..etc and starts Question answering,read  Langchain Document they explained each and every thing minutely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256be14-5db1-4665-8cc4-5745e113804e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
